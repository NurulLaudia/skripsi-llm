{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dd126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnlau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nnlau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\losses.py:2664: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nnlau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at D:\\Analisis Opini Skincare\\indobert_sentiment_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.33.131:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [20/Aug/2025 14:35:27] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "MODEL_PATH = r\"D:\\Analisis Opini Skincare\\indobert_sentiment_model\"\n",
    "MAX_LEN = 128\n",
    "LABELS = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "# ================== LOAD MODEL & TOKENIZER ==================\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# ================== FLASK APP ==================\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# ================== PREPROCESSING ==================\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  # remove mentions & links\n",
    "    text = re.sub(r'#\\w+', '', text)                  # remove hashtags\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', '', text)          # remove non-ascii\n",
    "    text = re.sub(r\"\\s\\s+\", \" \", text)                # remove multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# ================== SINGLE PREDICTION ==================\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    raw_text = request.form.get(\"text\")\n",
    "    if not raw_text:\n",
    "        return jsonify({\"error\": \"Missing text\"}), 400\n",
    "\n",
    "    clean_text = preprocess(raw_text)\n",
    "    encoded = tokenizer(\n",
    "        clean_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    # ================== PREDICTION ==================\n",
    "    logits = model(encoded)[0]                        # output logits\n",
    "    pred_probs = tf.nn.softmax(logits, axis=-1).numpy()  # convert to probabilities\n",
    "    pred_label = int(np.argmax(pred_probs, axis=1)[0])\n",
    "    pred_label_str = LABELS[pred_label]\n",
    "\n",
    "    return jsonify({\n",
    "        \"input\": raw_text,\n",
    "        \"cleaned\": clean_text,\n",
    "        \"predicted_label\": pred_label_str,\n",
    "        \"probabilities\": [float(x) for x in pred_probs[0]]\n",
    "    })\n",
    "\n",
    "# ================== HEALTH CHECK ==================\n",
    "@app.route('/', methods=['GET'])\n",
    "def home():\n",
    "    return jsonify({\"message\": \"IndoBERT Sentiment API is running!\"})\n",
    "\n",
    "# ================== RUN ==================\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dc910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at indobenchmark/indobert-base-p1 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nnlau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\saving\\legacy\\save.py:538: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nnlau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.20.10.4:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "\n",
    "MODEL_PATH = \"indobert_sentiment_model\"\n",
    "TOKENIZER_NAME = \"indobenchmark/indobert-base-p1\"\n",
    "MAX_LEN = 128\n",
    "LABELS = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "def build_model(bert_model_name, num_labels, max_len):\n",
    "    indobert = TFAutoModel.from_pretrained(bert_model_name)\n",
    "    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    bert_output = indobert({'input_ids': input_ids, 'attention_mask': attention_mask})[0]\n",
    "    cls_token = bert_output[:,0,:]\n",
    "    out = tf.keras.layers.Dense(num_labels, activation='softmax')(cls_token)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "num_labels = 3\n",
    "model = build_model(TOKENIZER_NAME, num_labels, MAX_LEN)\n",
    "model.load_weights(f\"{MODEL_PATH}/variables/variables\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "def preprocess(text):\n",
    "    def strip_emoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U0001F300-\\U0001F5FF\"\n",
    "            u\"\\U0001F680-\\U0001F6FF\"\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "            u\"\\U00002500-\\U00002BEF\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\u3030\"\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    def strip_all_entities(text):\n",
    "        text = text.replace('\\r', '').replace('\\n', ' ').lower()\n",
    "        text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "        text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "        banned_list = string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "        table = str.maketrans('', '', banned_list)\n",
    "        text = text.translate(table)\n",
    "        return text\n",
    "    def clean_hashtags(tweet):\n",
    "        tweet = re.sub(r'#\\w+\\b', '', tweet)\n",
    "        tweet = tweet.replace(\"#\", \"\")\n",
    "        return tweet\n",
    "    def filter_chars(text):\n",
    "        return \" \".join(\"\" if ('$' in word or '&' in word) else word for word in text.split())\n",
    "    def remove_mult_spaces(text):\n",
    "        return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "    text = strip_emoji(text)\n",
    "    text = strip_all_entities(text)\n",
    "    text = clean_hashtags(text)\n",
    "    text = filter_chars(text)\n",
    "    text = remove_mult_spaces(text)\n",
    "    return text.strip()\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    raw_text = request.form.get(\"text\")\n",
    "    if not raw_text:\n",
    "        return jsonify({\"error\": \"Missing text\"}), 400\n",
    "\n",
    "    clean_text = preprocess(raw_text)\n",
    "    encoded = tokenizer(\n",
    "        clean_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    pred_probs = model.predict({\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"]\n",
    "    })\n",
    "    pred_label = int(np.argmax(pred_probs, axis=1)[0])\n",
    "    pred_label_str = LABELS[pred_label]\n",
    "\n",
    "    return jsonify({\n",
    "        \"input\": raw_text,\n",
    "        \"cleaned\": clean_text,\n",
    "        \"predicted_label\": pred_label_str,\n",
    "        \"probabilities\": [float(x) for x in pred_probs[0]]\n",
    "    })\n",
    "\n",
    "@app.route('/predict_batch', methods=['POST'])\n",
    "def predict_batch():\n",
    "    # Accept file upload\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"error\": \"Missing file\"}), 400\n",
    "    file = request.files['file']\n",
    "    # Read file to DataFrame\n",
    "    try:\n",
    "        if file.filename.endswith('.csv'):\n",
    "            df = pd.read_csv(file)\n",
    "        elif file.filename.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file)\n",
    "        else:\n",
    "            return jsonify({\"error\": \"File must be .csv or .xlsx\"}), 400\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Error reading file: {str(e)}\"}), 400\n",
    "\n",
    "    # Kolom opini (boleh ganti ke 'opini', 'text', dsb sesuai format file)\n",
    "    col_candidates = ['opini', 'text', 'tweet', 'komentar']\n",
    "    for col in col_candidates:\n",
    "        if col in df.columns:\n",
    "            opini_col = col\n",
    "            break\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Kolom opini/tweet/text tidak ditemukan dalam file\"}), 400\n",
    "\n",
    "    results = []\n",
    "    texts = df[opini_col].fillna(\"\").astype(str).tolist()\n",
    "    cleaned_texts = [preprocess(t) for t in texts]\n",
    "    batch_enc = tokenizer(\n",
    "        cleaned_texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    pred_probs = model.predict({\n",
    "        \"input_ids\": batch_enc[\"input_ids\"],\n",
    "        \"attention_mask\": batch_enc[\"attention_mask\"]\n",
    "    })\n",
    "    pred_labels = np.argmax(pred_probs, axis=1)\n",
    "    for i, t in enumerate(texts):\n",
    "        results.append({\n",
    "            \"input\": t,\n",
    "            \"predicted_label\": LABELS[int(pred_labels[i])],\n",
    "            \"probabilities\": [float(x) for x in pred_probs[i]]\n",
    "        })\n",
    "    return jsonify({\"results\": results})\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def home():\n",
    "    return jsonify({\"message\": \"IndoBERT Sentiment API is running!\"})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
